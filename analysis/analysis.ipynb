{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2674a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('scheduler_metrics.csv')\n",
    "\n",
    "# Define metrics and their units\n",
    "metrics = {\n",
    "    'Turnaround_Time': 'Turnaround Time (k-ticks)',\n",
    "    'Waiting_Time': 'Waiting Time (k-ticks)',\n",
    "    'Response_Time': 'Response Time (k-ticks)',\n",
    "    'Context_Switches': 'Context Switches',\n",
    "    'CPU_Share': 'CPU Share (%)'\n",
    "}\n",
    "\n",
    "benchmarks = ['short_io', 'long_io', 'short_cpu', 'long_cpu_child']\n",
    "benchmark_labels = {\n",
    "    'short_io': 'Short I/O',\n",
    "    'long_io': 'Long I/O',\n",
    "    'short_cpu': 'Short CPU',\n",
    "    'long_cpu_child': 'Long CPU (4 procs avg)'\n",
    "}\n",
    "\n",
    "schedulers = ['RR', 'FIFO', 'EEVDF']\n",
    "colors = {'RR': '#3498db', 'FIFO': '#e74c3c', 'EEVDF': '#2ecc71'}\n",
    "\n",
    "# Create comparison plots for each metric\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "fig.suptitle('Scheduler Performance Comparison Across Benchmarks', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (metric_key, metric_label) in enumerate(metrics.items()):\n",
    "    if idx >= 5:  # We only have 5 metrics\n",
    "        break\n",
    "    \n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Prepare data for this metric\n",
    "    plot_data = []\n",
    "    for benchmark in benchmarks:\n",
    "        bench_data = df[df['Benchmark'] == benchmark]\n",
    "        for scheduler in schedulers:\n",
    "            sched_data = bench_data[bench_data['Scheduler'] == scheduler]\n",
    "            if not sched_data.empty:\n",
    "                plot_data.append({\n",
    "                    'Benchmark': benchmark_labels[benchmark],\n",
    "                    'Scheduler': scheduler,\n",
    "                    'Value': sched_data[metric_key].values[0]\n",
    "                })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    x = np.arange(len(benchmarks))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, scheduler in enumerate(schedulers):\n",
    "        sched_df = plot_df[plot_df['Scheduler'] == scheduler]\n",
    "        values = [sched_df[sched_df['Benchmark'] == benchmark_labels[b]]['Value'].values[0] \n",
    "                  if not sched_df[sched_df['Benchmark'] == benchmark_labels[b]].empty else 0\n",
    "                  for b in benchmarks]\n",
    "        \n",
    "        ax.bar(x + i * width, values, width, label=scheduler, color=colors[scheduler], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Benchmark Type', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(metric_label, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(metric_label, fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels([benchmark_labels[b] for b in benchmarks], rotation=15, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(metrics) < 6:\n",
    "    fig.delaxes(axes[2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scheduler_comparison_all_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create individual detailed plots for key metrics\n",
    "# 1. Response Time Comparison (Most Important for Interactivity)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "response_data = df.pivot(index='Benchmark', columns='Scheduler', values='Response_Time')\n",
    "response_data = response_data.reindex(benchmarks)\n",
    "response_data.index = [benchmark_labels[b] for b in benchmarks]\n",
    "\n",
    "x = np.arange(len(response_data.index))\n",
    "width = 0.25\n",
    "\n",
    "for i, scheduler in enumerate(schedulers):\n",
    "    ax.bar(x + i * width, response_data[scheduler], width, \n",
    "           label=scheduler, color=colors[scheduler], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Benchmark Type', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Response Time (k-ticks)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Response Time Comparison: Lower is Better for Interactivity', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(response_data.index)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, scheduler in enumerate(schedulers):\n",
    "    for j, v in enumerate(response_data[scheduler]):\n",
    "        if v > 0:\n",
    "            ax.text(j + i * width, v, f'{v:.1f}', \n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('response_time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Context Switches Comparison (Overhead Analysis)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ctx_data = df.pivot(index='Benchmark', columns='Scheduler', values='Context_Switches')\n",
    "ctx_data = ctx_data.reindex(benchmarks)\n",
    "ctx_data.index = [benchmark_labels[b] for b in benchmarks]\n",
    "\n",
    "x = np.arange(len(ctx_data.index))\n",
    "for i, scheduler in enumerate(schedulers):\n",
    "    ax.bar(x + i * width, ctx_data[scheduler], width, \n",
    "           label=scheduler, color=colors[scheduler], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Benchmark Type', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Context Switches', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Context Switches: Lower Indicates Less Overhead', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(ctx_data.index)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_yscale('log')  # Log scale for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('context_switches_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. CPU Share Comparison (Fairness)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "cpu_data = df.pivot(index='Benchmark', columns='Scheduler', values='CPU_Share')\n",
    "cpu_data = cpu_data.reindex(benchmarks)\n",
    "cpu_data.index = [benchmark_labels[b] for b in benchmarks]\n",
    "\n",
    "x = np.arange(len(cpu_data.index))\n",
    "for i, scheduler in enumerate(schedulers):\n",
    "    ax.bar(x + i * width, cpu_data[scheduler], width, \n",
    "           label=scheduler, color=colors[scheduler], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Benchmark Type', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('CPU Share (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('CPU Utilization: Higher is Better', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(cpu_data.index)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 105])\n",
    "\n",
    "# Add horizontal line at 100%\n",
    "ax.axhline(y=100, color='red', linestyle='--', alpha=0.5, label='Max (100%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cpu_share_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Summary Statistics Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    print(f\"\\n{benchmark_labels[benchmark]}:\")\n",
    "    print(\"-\" * 80)\n",
    "    bench_df = df[df['Benchmark'] == benchmark][['Scheduler', 'Response_Time', \n",
    "                                                   'Context_Switches', 'CPU_Share']]\n",
    "    print(bench_df.to_string(index=False))\n",
    "    \n",
    "    # Find best scheduler for each metric\n",
    "    best_response = bench_df.loc[bench_df['Response_Time'].idxmin(), 'Scheduler']\n",
    "    best_ctx = bench_df.loc[bench_df['Context_Switches'].idxmin(), 'Scheduler']\n",
    "    best_cpu = bench_df.loc[bench_df['CPU_Share'].idxmax(), 'Scheduler']\n",
    "    \n",
    "    print(f\"\\n  Best Response Time: {best_response}\")\n",
    "    print(f\"  Fewest Context Switches: {best_ctx}\")\n",
    "    print(f\"  Best CPU Share: {best_cpu}\")\n",
    "\n",
    "# 5. Fairness Analysis for Long CPU (detailed breakdown)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LONG CPU FAIRNESS ANALYSIS (4 Concurrent Processes)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate coefficient of variation (std/mean) for turnaround time\n",
    "# Lower CV = more fair\n",
    "fairness_data = {\n",
    "    'RR': {\n",
    "        'mean_turnaround': 24717.75,\n",
    "        'std_turnaround': np.std([23239.67, 24835.67, 25331, 26465.67]),\n",
    "        'cv': None\n",
    "    },\n",
    "    'FIFO': {\n",
    "        'mean_turnaround': 18694.33,\n",
    "        'std_turnaround': np.std([18500.33, 18579.67, 18701, 37102]),  # Note: huge variance!\n",
    "        'cv': None\n",
    "    },\n",
    "    'EEVDF': {\n",
    "        'mean_turnaround': 25733.08,\n",
    "        'std_turnaround': np.std([25373.33, 25564.33, 25899.67, 26095]),\n",
    "        'cv': None\n",
    "    }\n",
    "}\n",
    "\n",
    "for scheduler in schedulers:\n",
    "    cv = (fairness_data[scheduler]['std_turnaround'] / \n",
    "          fairness_data[scheduler]['mean_turnaround']) * 100\n",
    "    fairness_data[scheduler]['cv'] = cv\n",
    "    \n",
    "    print(f\"\\n{scheduler}:\")\n",
    "    print(f\"  Mean Turnaround: {fairness_data[scheduler]['mean_turnaround']:.2f} k-ticks\")\n",
    "    print(f\"  Std Dev: {fairness_data[scheduler]['std_turnaround']:.2f} k-ticks\")\n",
    "    print(f\"  Coefficient of Variation: {cv:.2f}% (lower = more fair)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Most Fair: {min(fairness_data, key=lambda x: fairness_data[x]['cv'])}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
